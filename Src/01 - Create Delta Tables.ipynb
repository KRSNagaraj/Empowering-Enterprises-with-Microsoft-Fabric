{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"3d59cee8-7c73-4ec9-a25c-fece7b2b3341"}],"default_lakehouse":"3d59cee8-7c73-4ec9-a25c-fece7b2b3341","default_lakehouse_name":"RetailSales","default_lakehouse_workspace_id":"8ab8bf61-69a5-4cf9-8cf9-b3667eb5d213"}}},"cells":[{"cell_type":"markdown","source":["### Spark session configuration\n","This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"53377b47-23c3-49b3-b3cf-915602c42c0f"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation.\n","# Licensed under the MIT License.\n","\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a31bdc3e-9d64-457a-afe5-362fe222eece","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-16T10:11:49.4070407Z","session_start_time":"2023-09-16T10:11:49.7066602Z","execution_start_time":"2023-09-16T10:11:58.7077683Z","execution_finish_time":"2023-09-16T10:12:01.0509662Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"38315bf8-704b-438e-a8b7-d2506d322cf6"},"text/plain":"StatementMeta(, a31bdc3e-9d64-457a-afe5-362fe222eece, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4f9e3699-f7f9-4e31-9fbe-850372e75b91"},{"cell_type":"markdown","source":["### Fact - Sale\n","\n","This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"98dc9627-1c0d-441e-90d2-366d878e3073"},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, quarter\n","\n","table_name = 'fact_sale'\n","\n","df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\n","df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n","df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n","df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a31bdc3e-9d64-457a-afe5-362fe222eece","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-16T10:12:22.8030968Z","session_start_time":null,"execution_start_time":"2023-09-16T10:12:23.2980752Z","execution_finish_time":"2023-09-16T10:13:48.4541292Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":7},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4724,"rowCount":50,"usageDescription":"","jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:13:46.558GMT","completionTime":"2023-09-16T10:13:46.604GMT","stageIds":[19,20,21],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4724,"dataRead":6156,"rowCount":57,"usageDescription":"","jobId":13,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:13:45.816GMT","completionTime":"2023-09-16T10:13:46.535GMT","stageIds":[17,18],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6156,"dataRead":10029,"rowCount":14,"usageDescription":"","jobId":12,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:13:45.536GMT","completionTime":"2023-09-16T10:13:45.612GMT","stageIds":[16],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":11,"name":"","description":"Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-09-16T10:13:44.885GMT","completionTime":"2023-09-16T10:13:44.885GMT","stageIds":[],"jobGroup":"5","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":225221050,"dataRead":562194776,"rowCount":100301686,"usageDescription":"","jobId":10,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-09-16T10:12:42.575GMT","completionTime":"2023-09-16T10:13:44.700GMT","stageIds":[15,14],"jobGroup":"5","status":"SUCCEEDED","numTasks":15,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":11,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":562194776,"dataRead":362243868,"rowCount":100301686,"usageDescription":"","jobId":9,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-09-16T10:12:24.470GMT","completionTime":"2023-09-16T10:12:42.513GMT","stageIds":[13],"jobGroup":"5","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":11,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":11,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":8,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/WW-Retail/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-09-16T10:12:23.709GMT","completionTime":"2023-09-16T10:12:24.084GMT","stageIds":[12],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"04f41858-8697-418c-ba5b-56165760c7fc"},"text/plain":"StatementMeta(, a31bdc3e-9d64-457a-afe5-362fe222eece, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4c45a7a2-a2b0-4e33-a5d6-88b08045f1d3"},{"cell_type":"markdown","source":["### Dimensions\n","This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02cdf0b1-e8f4-4c3b-9f18-c6b7211aa9a6"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","def loadFullDataFromSource(table_name):\n","    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","full_tables = [\n","    'dimension_city',\n","    'dimension_date',\n","    'dimension_employee',\n","    'dimension_stock_item'\n","    ]\n","\n","for table in full_tables:\n","    loadFullDataFromSource(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a31bdc3e-9d64-457a-afe5-362fe222eece","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-16T10:14:35.0157134Z","session_start_time":null,"execution_start_time":"2023-09-16T10:14:35.4595489Z","execution_finish_time":"2023-09-16T10:14:48.191721Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":28},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4633,"rowCount":50,"usageDescription":"","jobId":42,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:46.548GMT","completionTime":"2023-09-16T10:14:46.578GMT","stageIds":[60,61,59],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4633,"dataRead":2374,"rowCount":54,"usageDescription":"","jobId":41,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:46.025GMT","completionTime":"2023-09-16T10:14:46.528GMT","stageIds":[57,58],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":2374,"dataRead":3937,"rowCount":8,"usageDescription":"","jobId":40,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:45.822GMT","completionTime":"2023-09-16T10:14:45.875GMT","stageIds":[56],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":39,"name":"","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:45.275GMT","completionTime":"2023-09-16T10:14:45.275GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":21581,"dataRead":36536,"rowCount":1344,"usageDescription":"","jobId":38,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:44.831GMT","completionTime":"2023-09-16T10:14:45.168GMT","stageIds":[54,55],"jobGroup":"9","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":36536,"dataRead":56776,"rowCount":1344,"usageDescription":"","jobId":37,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:44.649GMT","completionTime":"2023-09-16T10:14:44.785GMT","stageIds":[53],"jobGroup":"9","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":36,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:44.320GMT","completionTime":"2023-09-16T10:14:44.383GMT","stageIds":[52],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4454,"rowCount":50,"usageDescription":"","jobId":35,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:44.158GMT","completionTime":"2023-09-16T10:14:44.193GMT","stageIds":[51,49,50],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4454,"dataRead":1802,"rowCount":54,"usageDescription":"","jobId":34,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:43.616GMT","completionTime":"2023-09-16T10:14:44.139GMT","stageIds":[48,47],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1802,"dataRead":2232,"rowCount":8,"usageDescription":"","jobId":33,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:43.397GMT","completionTime":"2023-09-16T10:14:43.478GMT","stageIds":[46],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":32,"name":"","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:42.792GMT","completionTime":"2023-09-16T10:14:42.792GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":7348,"dataRead":7711,"rowCount":426,"usageDescription":"","jobId":31,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:42.314GMT","completionTime":"2023-09-16T10:14:42.672GMT","stageIds":[45,44],"jobGroup":"9","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":7711,"dataRead":22728,"rowCount":426,"usageDescription":"","jobId":30,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:42.147GMT","completionTime":"2023-09-16T10:14:42.272GMT","stageIds":[43],"jobGroup":"9","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":29,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:41.881GMT","completionTime":"2023-09-16T10:14:41.941GMT","stageIds":[42],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4446,"rowCount":50,"usageDescription":"","jobId":28,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:41.712GMT","completionTime":"2023-09-16T10:14:41.752GMT","stageIds":[39,40,41],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4446,"dataRead":1934,"rowCount":54,"usageDescription":"","jobId":27,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:41.163GMT","completionTime":"2023-09-16T10:14:41.688GMT","stageIds":[37,38],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1934,"dataRead":3114,"rowCount":8,"usageDescription":"","jobId":26,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-09-16T10:14:40.970GMT","completionTime":"2023-09-16T10:14:41.022GMT","stageIds":[36],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":25,"name":"","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:40.414GMT","completionTime":"2023-09-16T10:14:40.414GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":68564,"dataRead":157900,"rowCount":12420,"usageDescription":"","jobId":24,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:39.910GMT","completionTime":"2023-09-16T10:14:40.275GMT","stageIds":[34,35],"jobGroup":"9","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":157900,"dataRead":157055,"rowCount":12420,"usageDescription":"","jobId":23,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/WW-Retail/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-09-16T10:14:39.511GMT","completionTime":"2023-09-16T10:14:39.867GMT","stageIds":[33],"jobGroup":"9","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"246c5b71-d48c-41e3-96d1-35494d2052ee"},"text/plain":"StatementMeta(, a31bdc3e-9d64-457a-afe5-362fe222eece, 9, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"49fdb15e-d514-4615-bb13-e792f484133b"}]}